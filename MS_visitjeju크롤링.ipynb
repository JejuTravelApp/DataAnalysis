{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium import webdriver  # 동적크롤링\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_scrapy_data : 전처리가 끝난 visitJeju의 API 전체 CSV\n",
    "data = pd.read_csv(\"Data/visit_jeju_scrapy_data.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>introduction</th>\n",
       "      <th>address</th>\n",
       "      <th>roadaddress</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>phoneno</th>\n",
       "      <th>imgpath</th>\n",
       "      <th>tag</th>\n",
       "      <th>contentsid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미유</td>\n",
       "      <td>쇼핑</td>\n",
       "      <td>숲속 별장처럼 꾸며진 소품 가게</td>\n",
       "      <td>제주특별자치도 제주시 한림읍 옹포리 326-3</td>\n",
       "      <td>제주특별자치도 제주시 한림읍 한림상로 15-5</td>\n",
       "      <td>33.405636</td>\n",
       "      <td>126.256762</td>\n",
       "      <td>0507-1349-9322</td>\n",
       "      <td>https://api.cdn.visitjeju.net/photomng/imgpath...</td>\n",
       "      <td>핸드메이드소품, 키링, 우산, 한림, 옹포리, 소품샵, 지갑, 쇼핑</td>\n",
       "      <td>CNTS_200000000015086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마야블루</td>\n",
       "      <td>쇼핑</td>\n",
       "      <td>마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다. 이...</td>\n",
       "      <td>제주특별자치도 제주시 노형동 1052-27</td>\n",
       "      <td>제주특별자치도 제주시 월랑로6길 21</td>\n",
       "      <td>33.489570</td>\n",
       "      <td>126.478593</td>\n",
       "      <td>010-8515-2470</td>\n",
       "      <td>https://api.cdn.visitjeju.net/photomng/imgpath...</td>\n",
       "      <td>악세사리, 쇼핑,라탄,원데이클래스,관광기념품,상점/상가, 쇼핑,라탄,원데이클래스,카...</td>\n",
       "      <td>CNTS_200000000007334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>몸냥공작소</td>\n",
       "      <td>쇼핑</td>\n",
       "      <td>귀엽고 제주스러운 유니크한 소품가게</td>\n",
       "      <td>제주특별자치도 제주시 애월읍 유수암리 2503-1</td>\n",
       "      <td>제주특별자치도 제주시 애월읍 하소로 595</td>\n",
       "      <td>33.430614</td>\n",
       "      <td>126.397470</td>\n",
       "      <td>--</td>\n",
       "      <td>https://api.cdn.visitjeju.net/photomng/imgpath...</td>\n",
       "      <td>공방,기념품,,아주 어려움, 공방,기념품,쇼핑,관광기념품,상점/상가</td>\n",
       "      <td>CNTS_000000000022837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>은인마켙</td>\n",
       "      <td>쇼핑</td>\n",
       "      <td>자개, 유리 등 다양한 식기류를 만나볼 수 있는 소품샵</td>\n",
       "      <td>제주특별자치도 제주시 조천읍 함덕리 1082</td>\n",
       "      <td>제주특별자치도 제주시 조천읍 함덕서2길 20</td>\n",
       "      <td>33.540661</td>\n",
       "      <td>126.663099</td>\n",
       "      <td>010-3006-7721</td>\n",
       "      <td>https://api.cdn.visitjeju.net/photomng/imgpath...</td>\n",
       "      <td>소품샵, 함덕, 잡화, 식기</td>\n",
       "      <td>CNTS_200000000015021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뱅뱅와인마켓</td>\n",
       "      <td>쇼핑</td>\n",
       "      <td>공항 인근에 위치한 대형 와인판매점</td>\n",
       "      <td>제주특별자치도 제주시 오라이동 2171-1</td>\n",
       "      <td>제주특별자치도 제주시 사평2길 9</td>\n",
       "      <td>33.492220</td>\n",
       "      <td>126.510559</td>\n",
       "      <td>064-746-4141</td>\n",
       "      <td>https://api.cdn.visitjeju.net/photomng/imgpath...</td>\n",
       "      <td>샴페인, 오라동, 와인, 제주시내</td>\n",
       "      <td>CNTS_200000000014989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    title category                                       introduction  \\\n",
       "0      미유       쇼핑                                  숲속 별장처럼 꾸며진 소품 가게   \n",
       "1    마야블루       쇼핑  마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다. 이...   \n",
       "2   몸냥공작소       쇼핑                                귀엽고 제주스러운 유니크한 소품가게   \n",
       "3    은인마켙       쇼핑                     자개, 유리 등 다양한 식기류를 만나볼 수 있는 소품샵   \n",
       "4  뱅뱅와인마켓       쇼핑                                공항 인근에 위치한 대형 와인판매점   \n",
       "\n",
       "                       address                roadaddress   latitude  \\\n",
       "0    제주특별자치도 제주시 한림읍 옹포리 326-3  제주특별자치도 제주시 한림읍 한림상로 15-5  33.405636   \n",
       "1      제주특별자치도 제주시 노형동 1052-27       제주특별자치도 제주시 월랑로6길 21  33.489570   \n",
       "2  제주특별자치도 제주시 애월읍 유수암리 2503-1    제주특별자치도 제주시 애월읍 하소로 595  33.430614   \n",
       "3     제주특별자치도 제주시 조천읍 함덕리 1082   제주특별자치도 제주시 조천읍 함덕서2길 20  33.540661   \n",
       "4      제주특별자치도 제주시 오라이동 2171-1         제주특별자치도 제주시 사평2길 9  33.492220   \n",
       "\n",
       "    longitude         phoneno  \\\n",
       "0  126.256762  0507-1349-9322   \n",
       "1  126.478593   010-8515-2470   \n",
       "2  126.397470              --   \n",
       "3  126.663099   010-3006-7721   \n",
       "4  126.510559    064-746-4141   \n",
       "\n",
       "                                             imgpath  \\\n",
       "0  https://api.cdn.visitjeju.net/photomng/imgpath...   \n",
       "1  https://api.cdn.visitjeju.net/photomng/imgpath...   \n",
       "2  https://api.cdn.visitjeju.net/photomng/imgpath...   \n",
       "3  https://api.cdn.visitjeju.net/photomng/imgpath...   \n",
       "4  https://api.cdn.visitjeju.net/photomng/imgpath...   \n",
       "\n",
       "                                                 tag            contentsid  \n",
       "0              핸드메이드소품, 키링, 우산, 한림, 옹포리, 소품샵, 지갑, 쇼핑  CNTS_200000000015086  \n",
       "1  악세사리, 쇼핑,라탄,원데이클래스,관광기념품,상점/상가, 쇼핑,라탄,원데이클래스,카...  CNTS_200000000007334  \n",
       "2              공방,기념품,,아주 어려움, 공방,기념품,쇼핑,관광기념품,상점/상가  CNTS_000000000022837  \n",
       "3                                    소품샵, 함덕, 잡화, 식기  CNTS_200000000015021  \n",
       "4                                 샴페인, 오라동, 와인, 제주시내  CNTS_200000000014989  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "\n",
    "# 여기서 이용할 컬럼은 contentsid 하나."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "category        0\n",
       "introduction    0\n",
       "address         0\n",
       "roadaddress     0\n",
       "latitude        0\n",
       "longitude       0\n",
       "phoneno         0\n",
       "imgpath         2\n",
       "tag             0\n",
       "contentsid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contentsid를 이용해 검색하여 해당 이름을 불러올 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNTS_200000000015086'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cocntentsid = df[['title','contentsid']]\n",
    "df_cocntentsid['contentsid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미유'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cocntentsid['title'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> contentsid를 통해 검색을 할것. 그러나 title이 뭔지는 알아야하니깐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검색할 항목 정리\n",
    "- https://www.visitjeju.net/kr/detail/view?contentsid=\n",
    "- df_contentsid['contentsid'][index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어떻게 추가할까?\n",
    "- df_contentsid 데이터프레임에 새로운 컬럼을 만들어 붙디던가,\n",
    "- 새로운 데이터 프레임을 만들어 크롤링한 데이터를 저장하고 df_contentsid랑 합치던가.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNTS_000000000019679'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cocntentsid['contentsid'][2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 할 데이터 정리\n",
    "- class \"inner_wrap\" 데이터(상단 데이터) - 상단 이미지 데이터의 경우 이미 있는 imgPath와 동일하기 때문에 필요 없음.\n",
    "    - title \"//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[1]/h3\"\n",
    "    - 별점 \"//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[3]/p\"\n",
    "    - mainTag(해당 관광지를 분류 가능) \n",
    "    \"//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[1]\"\n",
    "        //*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[1]/a[1]\n",
    "        //*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[1]/a[2]\n",
    "        //*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[1]/a[3]\n",
    "    - subTag(해당 관광지를 조금더 디테일하게 설명 )\n",
    "    \"//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[2]\"\n",
    "        //*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[2]/a[1]\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        //*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[2]/a[7]\n",
    "\n",
    "- class \"add2020_detail_left\" 데이터(본문내용 좌측데이터 = 상세정보)\n",
    "    - image 규칙이 없음. 있긴한데 div[3]에는 text가 있고 1,2,4,5에는 이미지가 있고, \n",
    "        - //*[@id=\"tab0\"]/div/div[1]/div[1]/div/div/div/div/img\n",
    "        - //*[@id=\"tab0\"]/div/div[1]/div[2]/div/div/div/div[1]/img\n",
    "        - //*[@id=\"tab0\"]/div/div[1]/div[2]/div/div/div/div[2]/img\n",
    "        - //*[@id=\"tab0\"]/div/div[1]/div[4]/div/div/div/div/img\n",
    "        - //*[@id=\"tab0\"]/div/div[1]/div[5]/div/div/div/div/img\n",
    "    - text\n",
    "        - 와 텍스트가 html언어네 이거 어케 긁어오지? 한번에 긁어올 수 있는 방법 필요.\n",
    "        //*[@id=\"tab0\"]/div/div[1]/div[3]/div/div/div\n",
    "\n",
    "- class \"add2020_detail_right\" 중 이용안내 파트만 긁어오기. 왜냐하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롬 켜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chrome Browser 와 Chrome Driver Version 확인하기\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()),options = chrome_options)\n",
    "# driver.get(f\"https://www.visitjeju.net/kr/detail/view?contentsid={df_cocntentsid['contentsid'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 연습코드\n",
    "# import requests\n",
    "# from lxml import html\n",
    "\n",
    "# # 크롤링할 웹페이지 URL 설정\n",
    "# url = 'https://www.visitjeju.net/kr/detail/view?contentsid=CNTS_000000000019279'\n",
    "# driver.get(url)\n",
    "# scrapy_data = pd.DataFrame(columns=['text1', 'image1'])\n",
    "\n",
    "# text1 = []\n",
    "# image1 = []\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     dl_tag = soup.find('dl')\n",
    "#     # 텍스트 가져오기\n",
    "#     text_element = soup.find('div', class_='wrap_contView')\n",
    "#     text = text_element.get_text(strip=True)\n",
    "    \n",
    "#     # 이미지 가져오기\n",
    "#     image_element = soup.find('div', class_='swiper-slide')\n",
    "#     image_url = image_element.find('img')['src']\n",
    "    \n",
    "#     # 데이터프레임에 저장\n",
    "#     data = {'text1': [text], 'image1': [image_url]}\n",
    "#     scrapy_data = pd.DataFrame(data)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # 크롤링할 웹페이지 URL 설정\n",
    "# url = 'https://www.visitjeju.net/kr/detail/view?contentsid=CNTS_000000000019279'\n",
    "\n",
    "# # 웹페이지에 GET 요청 보내고 HTML 가져오기\n",
    "# response = requests.get(url)\n",
    "# html = response.text\n",
    "\n",
    "# try:\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     # dl 태그 찾기\n",
    "#     dl_tag = soup.find('dl')\n",
    "    \n",
    "#     if dl_tag:\n",
    "#         # 텍스트와 이미지를 저장할 변수 초기화\n",
    "#         text1 = \"\"\n",
    "#         image1 = None\n",
    "        \n",
    "#         # dt와 dd 태그를 모두 찾아서 반복문으로 순회\n",
    "#         for dt_tag, dd_tag in zip(dl_tag.find_all('dt'), dl_tag.find_all('dd')):\n",
    "#             # dt 태그의 텍스트 추출 (제목)\n",
    "#             dt_text = dt_tag.get_text(strip=True)\n",
    "#             # dd 태그의 텍스트 추출 (상세 내용)\n",
    "#             dd_text = dd_tag.get_text(strip=True)\n",
    "            \n",
    "#             # 상세 내용에 이미지가 있는지 확인\n",
    "#             img_tag = dd_tag.find('img')\n",
    "#             if img_tag:\n",
    "#                 # 이미지의 src 속성 값 추출\n",
    "#                 image1 = img_tag.get('src')\n",
    "            \n",
    "#             # dt 태그의 텍스트와 dd 태그의 텍스트를 합쳐서 저장\n",
    "#             if dt_text:\n",
    "#                 text1 += f\"{dt_text}: {dd_text}\\n\"\n",
    "        \n",
    "#         # 데이터프레임에 저장\n",
    "#         data = {'text1': [text1], 'image1': [image1]}\n",
    "#         scrapy_data = pd.DataFrame(data)\n",
    "#     else:\n",
    "#         print(\"dl 태그를 찾을 수 없습니다.\")\n",
    "# except Exception as e:\n",
    "#     print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 크롤링할 웹페이지 URL 설정\n",
    "# url = 'https://www.visitjeju.net/kr/detail/view?contentsid=CNTS_000000000019279'\n",
    "# driver.get(url)\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "# # # dl 태그 찾기\n",
    "# # XPath로 dl 태그 찾기\n",
    "# dl_element = driver.find_element(By.XPATH,\"/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[2]/div[2]/div\")\n",
    "# try:\n",
    "#     # dt = title\n",
    "#     dt_elements = dl_element.find_elements(By.TAG_NAME, \"dt\")\n",
    "#     # dd = detail\n",
    "#     dd_elements = dl_element.find_elements(By.TAG_NAME, \"dd\")\n",
    "\n",
    "#     for dt, dd in zip(dt_elements, dd_elements):\n",
    "#         print(f\"{dt.text}-{dd.text}\")\n",
    "    \n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print('크롤링 실패')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div\n",
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div/dl/dt[1] #소개\n",
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div/dl/dd[1]/text() #관광지와 가까운 마니주펜션\n",
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div/dl/dt[2] #이용시간\n",
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div/dl/dd[2]/text()\n",
    "\n",
    "# //*[@id=\"content\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div/dl/dd[3] #공용주차장..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 크롤링할 웹페이지 URL 설정\n",
    "# url = 'https://www.visitjeju.net/kr/detail/view?contentsid=CNTS_000000000019279'\n",
    "# driver.get(url)\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "# # # dl 태그 찾기\n",
    "# # XPath로 dl 태그 찾기\n",
    "# # test_element = driver.find_element(By.XPATH, \"/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[2]/div[2]\")\n",
    "# dl_element = driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[2]/div[2]/div')\n",
    "# try:\n",
    "#     # dt = title\n",
    "#     dt_elements_save = []\n",
    "#     # dt_elements = dl_element.find_elements(By.TAG_NAME, \"//dl/dt\")\n",
    "#     dt_elements = dl_element.find_elements(By.XPATH, \"/dl/dt\")\n",
    "#     # dd = detail\n",
    "#     dd_elements_save = []\n",
    "#     dd_elements = dl_element.find_elements(By.XPATH, \"/dl/dd\")\n",
    "\n",
    "#     for idx, dt, dd in zip(dt_elements, dd_elements):\n",
    "#         idx + 1\n",
    "#         dt_elements_save.append(dt[idx])\n",
    "#         dd_elements_save.append(dd[idx])\n",
    "#         print(f\"{dt.text}-{dd.text}\")\n",
    "    \n",
    "\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print('크롤링 실패')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body Left 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/34c0934e-f750-49db-9a3a-7382581d8924.jpg\n",
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/05c35d7a-6be0-4e81-b93f-1919dce0b077.jpg\n",
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/90e8ed4a-91ae-4506-81ac-f67914d0fc63.jpg\n",
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/e97ea3a8-790a-4041-9953-c06f1f19ec29.jpg\n",
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/4f0a2f93-6e0a-47ca-a710-6ab9ff1168b1.jpg\n",
      "Image Link: https://api.cdn.visitjeju.net/photomng/imgpath/202306/13/0c7e5875-c0d5-4f63-96b6-0176dd4ce854.jpg\n",
      "Text: 가게 규모가 크진 않지만, 제주 여행 기념품 종류를 다양하게 구비하고 있어 둘러보는 재미가 쏠쏠한 편. 귤 모양 파우치, 스트링 백과 같은 사장님이 직접 만든 핸드메이드 제품도 선보인다. 여행객에게 인기 아이템인 모자의 경우 털, 뜨개실, 면 등 여러 소재가 구비되어 취향껏 고르기에 좋으며, 우산, 선글라스, 지갑, 키링과 같은 잡화류도 다양한 디자인이 준비되어 있다.\n",
      "Text: 이뿐 아니라 제주 특산물로 만든 다양한 먹거리들도 구비돼 있다. 스테디셀러인 초콜릿이나 과즐, 타르트 등은 선물용으로 인기가 좋다. 진열이 깔끔하게 돼 있고, 상품의 가짓수가 많아 둘러보는 재미가 쏠쏠하다.  제주여행 기념품 쇼핑을 하고 싶을 때 방문할 만한 곳이다.\n",
      "Text: 미유는 일몰의 풍경이 아름다운 것으로 유명한 한림항 인근에 있는 소품 가게이다. 멀리서도 눈에 띄는 나무건물은 빈티지한 분위기를 자아낸다. 노란색 간판에 가게의 상호가 쓰여 있다. 입구를 열고 들어서면 마치 별장처럼 꾸며진 공간이 눈길을 끈다. 인테리어는 전체적으로 나무를 활용하여 인위적이지 않고 자연스러운 느낌이다.\n",
      "Text: 숲속 별장처럼 꾸며진 소품 가게\n"
     ]
    }
   ],
   "source": [
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "# # 요소들을 가져올 XPath\n",
    "# xpath = '/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[1]/div[1]/div/div[1]//*'\n",
    "\n",
    "# # 이미지와 텍스트를 모두 가져오는 함수\n",
    "# def get_images_and_texts(driver, xpath):\n",
    "#     elements = driver.find_elements(By.XPATH, xpath)\n",
    "#     images = []\n",
    "#     texts = []\n",
    "\n",
    "#     for element in elements:\n",
    "#         if element.tag_name == 'img':\n",
    "#             images.append(element.get_attribute('src'))\n",
    "#         else:\n",
    "#             # p 태그 안에 있는 text만 가져오기\n",
    "#             p_elements = element.find_elements(By.XPATH, './/p')\n",
    "#             for p_element in p_elements:\n",
    "#                 text = p_element.text.strip()\n",
    "#                 if text:\n",
    "#                     texts.append(text)\n",
    "#     # 중복된 항목 제거\n",
    "#     unique_texts = list(set(texts))\n",
    "#     return images, unique_texts\n",
    "\n",
    "# # 이미지와 텍스트 가져오기\n",
    "# images, texts = get_images_and_texts(driver, xpath)\n",
    "\n",
    "# # 결과 출력\n",
    "# for image in images:\n",
    "#     print(\"Image Link:\", image)\n",
    "\n",
    "# for text in texts:\n",
    "#     print(\"Text:\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome Browser 와 Chrome Driver Version 확인하기\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()),options = chrome_options)\n",
    "driver.get(f\"https://www.visitjeju.net/kr/detail/view?contentsid={df_cocntentsid['contentsid'][0]}\")\n",
    "# driver.get(f\"https://www.visitjeju.net/kr/detail/view?contentsid=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'페이지를 찾을 수 없습니다.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div[2]/div/div/div/div/div/p').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================현재 page : 724, title : 종종제주====================\n",
      "추천 버튼이 없습니다.\n",
      "\n",
      "종종제주 - 별점(별점없음)\n",
      "#조천, #선흘리, #소품샵\n",
      "#잡화,#문구류,#핸드메이드소품,#쇼핑\n",
      "0BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/b27992b2-3558-40d9-9857-841397c229d7.jpg\n",
      "1BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/fe5b3e24-df69-471e-a373-7682075db99f.jpg\n",
      "2BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/25dfe2fd-ac91-4d0e-bd22-5d42750a78b1.jpg\n",
      "3BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/f7088fb9-c648-4cc1-9861-62c062911e27.jpg\n",
      "4BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/98677927-5cda-44c8-94a5-98ddb58b84d5.jpg\n",
      "5BodyLeftImage: https://api.cdn.visitjeju.net/photomng/imgpath/202306/15/0b7e12e7-02e3-4aa3-9449-ef520cfe64bd.jpg\n",
      "0BodyLeftText: 종종제주는 조용한 중산간 마을에 있는 소품숍이다. 푸른 나무들이 가득한 산간 지역에 반짝이는 은 색깔 건물 외관이 한눈에 들어온다.한적한 마을에 있어 여유로운 쇼핑이 가능하다. 가게 외벽에는 익살스러운 표정의 오리가 손님을 반긴다. 커다란 유리창을 통해 제주도의 숲을 보며 여유롭게 쇼핑할 수 있는 곳이다. 매장 내부 역시 은색으로 통일감을 준다.\n",
      "1BodyLeftText: 종종제주의 베스트셀러는 동물 모양의 접시와 캐릭터가 그려진 유리잔이다. 사장님의 취향이 듬뿍 담긴 큐레이션 덕분에, 귀여운 일러스트 캐릭터 상품이나 개성 넘치는 굿즈를 만나볼 수 있다. 제주여행의 뻔한 기념품에 질린 사람이라면 한 번쯤 방문할 만한 곳이다. 이곳만의 특색 넘치는 소품을 구경하는 것만으로 즐거운 시간이 된다.\n",
      "2BodyLeftText: 아주 크지도, 작지도 않은 공간에 여러 가지 종류의 개성 가득한 상품을 선보이고 있다. 특히 유명한 핸드메이드 작가의 작품을 위주로 진열해 놓아 다른 숍에서는 평소 볼 수 없는 것이 많은 편이다. 방문 전, 종종제주 인스타그램에 올라온 새로운 입고 상품을 확인해 보고 가는 것도 방법이다.\n",
      "bodyRightText : 소개-조용한 산간 마을에서 만나는 아기자기한 제주소품샵\n",
      "bodyRightText : 이용 시간-평일 : 10:30 ~ 18:00 , 주말 : 10:30 ~ 18:00\n",
      "=================현재 page : 725, title : 고씨주택====================\n",
      "추천 버튼이 없습니다.\n",
      "에러페이지\n",
      "=================현재 page : 726, title : 별도포구====================\n",
      "추천 버튼이 없습니다.\n",
      "\n",
      "별도포구 - 별점(별점없음)\n",
      "#포구, #해변, #문화유적지\n",
      "#자연경관,#역사유적\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# 이미지와 텍스트 가져오기\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     bodyLeftImages, unique_texts \u001b[38;5;241m=\u001b[39m get_images_and_texts(driver, bodyLeftSelector)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bodyLeftImages):\n",
      "Cell \u001b[0;32mIn[31], line 114\u001b[0m, in \u001b[0;36mget_images_and_texts\u001b[0;34m(driver, bodyLeftSelector)\u001b[0m\n\u001b[1;32m    111\u001b[0m elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, bodyLeftSelector)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m element\u001b[38;5;241m.\u001b[39mtag_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    115\u001b[0m         bodyLeftImages\u001b[38;5;241m.\u001b[39mappend(element\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;66;03m# p 태그 안에 있는 text만 가져오기\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:85\u001b[0m, in \u001b[0;36mWebElement.tag_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtag_name\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This element's ``tagName`` property.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mGET_ELEMENT_TAG_NAME)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:74\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_url\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/request.py:96\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[1;32m     94\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:415\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    413\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1284\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1002\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1000\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.send\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msendall(data)\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 본 코드\n",
    "mainUrl= \"https://www.visitjeju.net/kr/detail/view?contentsid=\"\n",
    "contentsid = df_cocntentsid['contentsid']\n",
    "# 크롤링 설정\n",
    "idx = 0 # 크롤링할 데이터 총 갯수\n",
    "failed_idx = 0 # 크롤링 실패시 1 증가\n",
    "\n",
    "t = random.randrange(2,3)\n",
    "\n",
    "# 크롤링할 데이터 담길 곳(header, body_left, body_right)\n",
    "    # header (title, stars, mainTag, subTag)\n",
    "scrapy_header = pd.DataFrame(columns=['title', 'stars', 'mainTag', 'subTag'])\n",
    "    # body_left를 image, text로 나눔 image는 개발자 사용, text는 분석가 사용\n",
    "scrapy_left_image = pd.DataFrame(columns=['title', 'image'])\n",
    "scrapy_left_text = pd.DataFrame(columns=['title', 'text'])\n",
    "    # body_right (subInformTitle == dt, subInformText == dd)\n",
    "scrapy_right_개발 = pd.DataFrame(columns=['title','BodyRightText'])\n",
    "scrapy_right_분석 = pd.DataFrame(columns=['title', 'BodyRightText'])\n",
    "\n",
    "\n",
    "# 본문 크롤링\n",
    "# for index, contentName in enumerate(contentsid[724:], start=724):\n",
    "for index, contentName in enumerate(contentsid):\n",
    "    print(f\"=================현재 page : {index}, title : {df_cocntentsid['title'][index]}====================\")\n",
    "    time.sleep(t)\n",
    "    driver.get(f\"{mainUrl}{contentsid[index]}\")\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 추천 버튼 뜨면 클릭\n",
    "    try:\n",
    "        # 추천 버튼이 있는지 확인\n",
    "        recommend_button = driver.find_element(By.XPATH, '//*[@id=\"footer\"]/div[4]/button')\n",
    "        \n",
    "        # 추천 버튼이 있을 경우 클릭\n",
    "        if recommend_button.is_displayed():\n",
    "            recommend_button.click()\n",
    "            print(\"===================버튼클릭완료 ===================\")\n",
    "            time.sleep(t)\n",
    "    except Exception as e:\n",
    "        print(\"추천 버튼이 없습니다.\")\n",
    "\n",
    "    # 에러페이지 뜨면 다음으로 넘어가기\n",
    "    try:\n",
    "        if driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/div/div/p').text == '페이지를 찾을 수 없습니다.':\n",
    "            print(\"에러페이지\")\n",
    "            scrapy_header.loc[len(scrapy_header)] = ['없음', '없음', '없음', '없음']\n",
    "            scrapy_left_image.loc[len(scrapy_left_image)] = ['없음', '없음']\n",
    "            scrapy_left_text.loc[len(scrapy_left_text)] = ['없음', '없음']\n",
    "            scrapy_right_개발.loc[len(scrapy_right_개발)] = ['없음', '없음']\n",
    "            scrapy_right_분석.loc[len(scrapy_right_분석)] = ['없음', '없음']\n",
    "            continue\n",
    "            \n",
    "    except :\n",
    "        print(\"\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # ========================header 크롤링=======================\n",
    "    # title, stars, mainTag, subTag\n",
    "    headerSelector = driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[1]/div[1]')\n",
    "    try:\n",
    "        header_title = headerSelector.find_elements(By.XPATH, '//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[1]/h3')\n",
    "        header_stars = headerSelector.find_elements(By.XPATH, '//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[3]/p')\n",
    "        header_mainTag = headerSelector.find_elements(By.CLASS_NAME, \"best_tag\")\n",
    "        header_subTag = headerSelector.find_elements(By.XPATH, '//*[@id=\"content\"]/div[2]/div[1]/div[1]/div[2]/div[4]/p[2]')\n",
    "        # title, stars\n",
    "        for h_title, h_stars in zip(header_title, header_stars):\n",
    "            print(h_title.text,\"-\", h_stars.text)\n",
    "            # scrapy_header[['title']] = pd.concat({'title': h_title.text},ignore_index=True)\n",
    "            # scrapy_header[['stars']] = pd.concat({'stars': h_stars.text},ignore_index=True)\n",
    "\n",
    "        # mainTag\n",
    "        for midx, h_mainTag in enumerate(header_mainTag):\n",
    "            # 태그 분리\n",
    "            tags = h_mainTag.text.split(\"#\")\n",
    "            tags = [f\"#{tag.strip()}\" for tag in tags if tag.strip()]  # \"#\"을 다시 추가하여 태그 리스트 작성\n",
    "            print(\", \".join(tags))  # 분리된 태그들을 출력        \n",
    "            # scrapy_header[['mainTag']] = pd.concat({'mainTag': \", \".join(tags)},ignore_index=True)\n",
    "            \n",
    "        # subTag\n",
    "        for midx, h_subTag in enumerate(header_subTag):\n",
    "            stags = h_subTag.find_elements(By.TAG_NAME, 'a')\n",
    "            ###안되면 아래두줄 살리기\n",
    "            # for stag in stags:\n",
    "            #     print(f\"sub태그{index} : {stag.text}\")\n",
    "            stags = h_subTag.text.split(\"#\")\n",
    "            stags = [f\"#{stag.strip()}\" for stag in stags if stag.strip()]\n",
    "            print(\",\".join(stags))\n",
    "            # scrapy_header[['subTag']] = pd.concat({'subTag': \", \".join(stags)},ignore_index=True)\n",
    "        \n",
    "        # 헤더 데이터 저장\n",
    "        for h_title, h_stars, h_mainTag, h_subTag in zip(header_title, header_stars, header_mainTag, header_subTag):\n",
    "            scrapy_header.loc[len(scrapy_header)] = [h_title.text, h_stars.text, h_mainTag.text if h_mainTag else None, h_subTag.text if h_subTag else None]\n",
    "            \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"헤더 크롤링 실패\")\n",
    "        continue\n",
    "    \n",
    "    time.sleep(t)\n",
    "    # ========================bodyLeft크롤링=======================\n",
    "    # bodyLeftText, bodyLeftImage\n",
    "        # image 링크 예시\n",
    "        # api.cdn.visitjeju.net/photomng/imgpath/202306/15/05dc24f9-60b7-415b-bf27-57361e59147d.jpg\n",
    "    bodyLeftSelector = '/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[1]/div[1]/div/div[1]//*'\n",
    "    bodyLeftImages = []\n",
    "    bodyLeftTexts = []\n",
    "    # 이미지와 텍스트를 모두 가져오는 함수\n",
    "    def get_images_and_texts(driver, bodyLeftSelector):\n",
    "        elements = driver.find_elements(By.XPATH, bodyLeftSelector)\n",
    "\n",
    "        for element in elements:\n",
    "            if element.tag_name == 'img':\n",
    "                bodyLeftImages.append(element.get_attribute('src'))\n",
    "            else:\n",
    "                # p 태그 안에 있는 text만 가져오기\n",
    "                p_elements = element.find_elements(By.XPATH, './/p')\n",
    "                for p_element in p_elements:\n",
    "                    text = p_element.text.strip()\n",
    "                    if text:\n",
    "                        bodyLeftTexts.append(text)\n",
    "        # 중복된 항목 제거\n",
    "        unique_texts = list(set(bodyLeftTexts))\n",
    "        return bodyLeftImages, unique_texts\n",
    "\n",
    "    # 이미지와 텍스트 가져오기\n",
    "    try:\n",
    "        bodyLeftImages, unique_texts = get_images_and_texts(driver, bodyLeftSelector)\n",
    "        # 결과 출력\n",
    "        for idx, image in enumerate(bodyLeftImages):\n",
    "            print(f\"{idx}BodyLeftImage:\", image)\n",
    "            # scrapy_left_image에 이미지 추가\n",
    "            scrapy_left_image.loc[len(scrapy_left_image)] = [f\"{h_title.text}{idx}\", image]\n",
    "            \n",
    "\n",
    "        for idx, text in enumerate(unique_texts):\n",
    "            print(f\"{idx}BodyLeftText:\", text)\n",
    "            # scrapy_left_text.loc[len(scrapy_left_text)] = [h_title.text, text]\n",
    "        combined_text = ' '.join(unique_texts)  # 모든 텍스트를 공백으로 구분하여 하나의 문자열로 합침\n",
    "        scrapy_left_text.loc[len(scrapy_left_text)] = [h_title.text, combined_text]  # 한 행으로 추가\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"bodyLeft크롤링 실패\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    # ========================bodyRight 크롤링=======================\n",
    "    # dl 태그 찾기\n",
    "    dl_element = driver.find_element(By.XPATH,\"/html/body/div/div[2]/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div[2]/div[2]/div\")\n",
    "    try:\n",
    "        # dt = title\n",
    "        dt_elements = dl_element.find_elements(By.TAG_NAME, \"dt\")\n",
    "        # dd = detail\n",
    "        dd_elements = dl_element.find_elements(By.TAG_NAME, \"dd\")\n",
    "        time.sleep(1)\n",
    "        # dt_elements와 dd_elements의 text 출력\n",
    "        for dt, dd in zip(dt_elements, dd_elements):\n",
    "            print(f'bodyRightText : {dt.text}-{dd.text}')\n",
    "            scrapy_right_개발.loc[len(scrapy_right_개발)] = [f\"{h_title.text}\",f\"{dt.text}-{dd.text}\"]\n",
    "            scrapy_right_분석 = scrapy_right_개발.groupby('title')['BodyRightText'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "            scrapy_right_분석 = scrapy_right_분석.set_index('title').reindex(scrapy_header['title']).reset_index()\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('bodyRight크롤링 실패')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>종종제주</td>\n",
       "      <td>종종제주는 조용한 중산간 마을에 있는 소품숍이다. 푸른 나무들이 가득한 산간 지역에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>없음</td>\n",
       "      <td>없음</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title                                               text\n",
       "0  종종제주  종종제주는 조용한 중산간 마을에 있는 소품숍이다. 푸른 나무들이 가득한 산간 지역에...\n",
       "1    없음                                                 없음"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrapy_header\n",
    "scrapy_left_image\n",
    "scrapy_left_text.head()\n",
    "# scrapy_right_개발"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>BodyRightText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미유</td>\n",
       "      <td>소개-숲속 별장처럼 꾸며진 소품 가게, 이용 시간-평일 : 11:00 ~ 18:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마야블루</td>\n",
       "      <td>소개-마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title                                      BodyRightText\n",
       "0    미유  소개-숲속 별장처럼 꾸며진 소품 가게, 이용 시간-평일 : 11:00 ~ 18:00...\n",
       "1  마야블루  소개-마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다..."
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapy_right_분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "      <th>mainTag</th>\n",
       "      <th>subTag</th>\n",
       "      <th>text</th>\n",
       "      <th>BodyRightText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미유</td>\n",
       "      <td></td>\n",
       "      <td>#한림 #옹포리 #소품샵</td>\n",
       "      <td>#핸드메이드소품 #키링 #우산 #지갑 #쇼핑</td>\n",
       "      <td>가게 규모가 크진 않지만, 제주 여행 기념품 종류를 다양하게 구비하고 있어 둘러보는...</td>\n",
       "      <td>소개-숲속 별장처럼 꾸며진 소품 가게, 이용 시간-평일 : 11:00 ~ 18:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>마야블루</td>\n",
       "      <td></td>\n",
       "      <td>#쇼핑 #라탄 #원데이클래스</td>\n",
       "      <td>#관광기념품 #상점/상가</td>\n",
       "      <td>제주에는 작지만 매력적인 공간들이 꽤나 많다. 마야블루는 제주 시내의 주택가 사이에...</td>\n",
       "      <td>소개-마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title stars          mainTag                    subTag  \\\n",
       "0    미유          #한림 #옹포리 #소품샵  #핸드메이드소품 #키링 #우산 #지갑 #쇼핑   \n",
       "1  마야블루        #쇼핑 #라탄 #원데이클래스             #관광기념품 #상점/상가   \n",
       "\n",
       "                                                text  \\\n",
       "0  가게 규모가 크진 않지만, 제주 여행 기념품 종류를 다양하게 구비하고 있어 둘러보는...   \n",
       "1  제주에는 작지만 매력적인 공간들이 꽤나 많다. 마야블루는 제주 시내의 주택가 사이에...   \n",
       "\n",
       "                                       BodyRightText  \n",
       "0  소개-숲속 별장처럼 꾸며진 소품 가게, 이용 시간-평일 : 11:00 ~ 18:00...  \n",
       "1  소개-마야블루는 제주 시내의 주택가 사이에 작은 간판으로 자신의 존재를 알리고 있다...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 합치기\n",
    "# scrapy_header와 scrapy_left_text 병합\n",
    "scrapy_combined_data = pd.merge(scrapy_header, scrapy_left_text, on='title', how='inner')\n",
    "scrapy_combined_data = pd.merge(scrapy_combined_data, scrapy_right_분석, on='title', how='inner')\n",
    "scrapy_combined_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 저장(분석용)\n",
    "scrapy_combined_data.to_csv(\"Data/visitJeju 크롤링.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장\n",
    "scrapy_left_image.to_csv(\"Data/visitJeju 이미지크롤링.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디테일 뷰에 쓸 소개글 저장\n",
    "scrapy_left_text.to_csv(\"Data/visitJeju 디테일뷰에 사용할 설명.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
